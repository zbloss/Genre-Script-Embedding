{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from plotly.offline import iplot, plot, init_notebook_mode\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "print(\"GPU Available: \", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/django_unchained_script.txt', 'r', encoding='utf8') as file:\n",
    "    django = file.read()\n",
    "    file.close()\n",
    "    \n",
    "with open('../data/inglorious_basterds_script.txt', encoding='utf8') as file:\n",
    "    ingbast = file.read()\n",
    "    file.close()\n",
    "    \n",
    "with open('../data/pulp_fiction_script.txt', 'r', encoding='utf8') as file:\n",
    "    pulp = file.read()\n",
    "    file.close()\n",
    "    \n",
    "with open('../data/reservoir_dogs_screenplay.txt', 'r', encoding='utf8') as file:\n",
    "    dogs = file.read()\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looks like these scripts have character names and other key details in all Caps. \n",
    "\n",
    "## Removing these words to help obscure the text from the film. \n",
    "\n",
    "## Also removing punctuation.\n",
    "\n",
    "## Also Lemmatizing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(line):\n",
    "    line = re.sub(r'\\b[A-Z]+\\b', '', line)\n",
    "    line = re.sub(r'[^\\w\\s]','', line)\n",
    "    line = line.lower()\n",
    "    line = ' '.join(line.split())\n",
    "    \n",
    "    lemma_line = []\n",
    "    line = nlp(line)\n",
    "    for word in line:\n",
    "        lemma_line.append(word.lemma_)\n",
    "    \n",
    "    #lemma_line = [l for l in lemma_line]\n",
    "    return ' '.join(lemma_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "django = text_cleaner(django)\n",
    "ingbast = text_cleaner(ingbast)\n",
    "pulp = text_cleaner(pulp)\n",
    "dogs = text_cleaner(dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to batch these into multiple dataframes\n",
    "\n",
    "def line_splitter(line, input_len=25):\n",
    "    split_line = line.split()\n",
    "    line_list = []\n",
    "    for i in range(0, len(line), input_len):\n",
    "        line_list.append(split_line[i:i+input_len])\n",
    "        \n",
    "    return line_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "django = pd.DataFrame([line_splitter(django)]).T\n",
    "ingbast = pd.DataFrame([line_splitter(ingbast)]).T\n",
    "pulp = pd.DataFrame([line_splitter(pulp)]).T\n",
    "dogs = pd.DataFrame([line_splitter(dogs)]).T\n",
    "\n",
    "\n",
    "django.columns = ingbast.columns = pulp.columns = dogs.columns = ['line'] #['index', 'line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict = {\n",
    "    'Django Unchained': 0,\n",
    "    'Inglorius Bastards': 1,\n",
    "    'Resevoir Dogs': 2,\n",
    "    'Pulp Fiction': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "django['film'] = 0\n",
    "ingbast['film'] = 1\n",
    "dogs['film'] = 2\n",
    "pulp['film'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([django, ingbast, dogs, pulp])\n",
    "\n",
    "df = df[df.astype(str)['line'] != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1400\n",
       "1    1166\n",
       "3    1011\n",
       "2     708\n",
       "Name: film, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looks like these are relatively even, Not as much for inglorious bastards as the others, but it should be fine.\n",
    "df['film'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin to assemble word2idx and idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['line'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "all_text = ' '.join(df['text'].values.tolist())\n",
    "all_test_l = list(set(all_text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6683 unique vocab words\n"
     ]
    }
   ],
   "source": [
    "# all_test_l contains a unique list of words used in all of the texts\n",
    "print(f'{len(all_test_l)} unique vocab words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "\n",
    "for i in range(len(all_test_l)):\n",
    "    idx2word[i+2] = all_test_l[i]\n",
    "    word2idx[idx2word[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx_mapper(line):\n",
    "    idx_list = []\n",
    "    for word in line:\n",
    "        try:\n",
    "            idx_list.append(word2idx[word])\n",
    "        except:\n",
    "            idx_list.append(word2idx['<UNK>'])\n",
    "            pass\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df['line'].map(word2idx_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>film</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[write, by, quentin, tarantino, as, the, film,...</td>\n",
       "      <td>0</td>\n",
       "      <td>write by quentin tarantino as the film play co...</td>\n",
       "      <td>[4545, 889, 5521, 2727, 5700, 3525, 3985, 3943...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[location, be, somewhere, in, texas, the, blac...</td>\n",
       "      <td>0</td>\n",
       "      <td>location be somewhere in texas the black man a...</td>\n",
       "      <td>[2516, 3977, 2508, 2948, 5446, 3525, 5566, 520...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[be, two, slave, trader, call, the, and, one, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>be two slave trader call the and one of the se...</td>\n",
       "      <td>[3977, 2305, 447, 3398, 2518, 3525, 4278, 6568...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[or, may, not, notice, a, tiny, small, r, burn...</td>\n",
       "      <td>0</td>\n",
       "      <td>or may not notice a tiny small r burn into -PR...</td>\n",
       "      <td>[2732, 6490, 981, 179, 6126, 2836, 5159, 584, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[have, be, by, bull, whip, beating, as, the, o...</td>\n",
       "      <td>0</td>\n",
       "      <td>have be by bull whip beating as the operatic o...</td>\n",
       "      <td>[3414, 3977, 889, 3193, 453, 1238, 5700, 3525,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                line  film  \\\n",
       "0  [write, by, quentin, tarantino, as, the, film,...     0   \n",
       "1  [location, be, somewhere, in, texas, the, blac...     0   \n",
       "2  [be, two, slave, trader, call, the, and, one, ...     0   \n",
       "3  [or, may, not, notice, a, tiny, small, r, burn...     0   \n",
       "4  [have, be, by, bull, whip, beating, as, the, o...     0   \n",
       "\n",
       "                                                text  \\\n",
       "0  write by quentin tarantino as the film play co...   \n",
       "1  location be somewhere in texas the black man a...   \n",
       "2  be two slave trader call the and one of the se...   \n",
       "3  or may not notice a tiny small r burn into -PR...   \n",
       "4  have be by bull whip beating as the operatic o...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [4545, 889, 5521, 2727, 5700, 3525, 3985, 3943...  \n",
       "1  [2516, 3977, 2508, 2948, 5446, 3525, 5566, 520...  \n",
       "2  [3977, 2305, 447, 3398, 2518, 3525, 4278, 6568...  \n",
       "3  [2732, 6490, 981, 179, 6126, 2836, 5159, 584, ...  \n",
       "4  [3414, 3977, 889, 3193, 453, 1238, 5700, 3525,...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tokenized']\n",
    "y = df['film']\n",
    "\n",
    "maxlen = 25\n",
    "\n",
    "X = pad_sequences(\n",
    "    X,\n",
    "    maxlen=maxlen,\n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, Embedding, Dense, LSTM, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 25, 32)            213856    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 267,460\n",
      "Trainable params: 267,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3428 samples, validate on 857 samples\n",
      "Epoch 1/30\n",
      "3428/3428 [==============================] - ETA: 36s - loss: 1.3859 - acc: 0.28 - ETA: 18s - loss: 1.3848 - acc: 0.33 - ETA: 12s - loss: 1.3830 - acc: 0.35 - ETA: 8s - loss: 1.3811 - acc: 0.3750 - ETA: 7s - loss: 1.3791 - acc: 0.384 - ETA: 5s - loss: 1.3775 - acc: 0.385 - ETA: 4s - loss: 1.3751 - acc: 0.385 - ETA: 4s - loss: 1.3727 - acc: 0.382 - ETA: 3s - loss: 1.3700 - acc: 0.382 - ETA: 3s - loss: 1.3665 - acc: 0.380 - ETA: 2s - loss: 1.3556 - acc: 0.391 - ETA: 2s - loss: 1.3498 - acc: 0.393 - ETA: 1s - loss: 1.3380 - acc: 0.392 - ETA: 1s - loss: 1.3286 - acc: 0.391 - ETA: 1s - loss: 1.3192 - acc: 0.395 - ETA: 0s - loss: 1.3155 - acc: 0.396 - ETA: 0s - loss: 1.3034 - acc: 0.398 - ETA: 0s - loss: 1.3000 - acc: 0.397 - ETA: 0s - loss: 1.2956 - acc: 0.401 - ETA: 0s - loss: 1.2872 - acc: 0.406 - 3s 892us/step - loss: 1.2799 - acc: 0.4049 - val_loss: 2.9134 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 1.1649 - acc: 0.414 - ETA: 1s - loss: 1.1766 - acc: 0.429 - ETA: 1s - loss: 1.2048 - acc: 0.434 - ETA: 1s - loss: 1.2013 - acc: 0.423 - ETA: 1s - loss: 1.1878 - acc: 0.431 - ETA: 1s - loss: 1.1840 - acc: 0.407 - ETA: 0s - loss: 1.1807 - acc: 0.407 - ETA: 0s - loss: 1.1740 - acc: 0.404 - ETA: 0s - loss: 1.1693 - acc: 0.404 - ETA: 0s - loss: 1.1643 - acc: 0.408 - ETA: 0s - loss: 1.1558 - acc: 0.406 - ETA: 0s - loss: 1.1555 - acc: 0.408 - ETA: 0s - loss: 1.1564 - acc: 0.404 - ETA: 0s - loss: 1.1502 - acc: 0.409 - ETA: 0s - loss: 1.1419 - acc: 0.409 - ETA: 0s - loss: 1.1331 - acc: 0.410 - ETA: 0s - loss: 1.1297 - acc: 0.413 - 1s 432us/step - loss: 1.1218 - acc: 0.4128 - val_loss: 2.5857 - val_acc: 0.0175\n",
      "Epoch 3/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 0.9119 - acc: 0.460 - ETA: 1s - loss: 0.8877 - acc: 0.541 - ETA: 1s - loss: 0.8826 - acc: 0.552 - ETA: 1s - loss: 0.8755 - acc: 0.554 - ETA: 0s - loss: 0.8674 - acc: 0.575 - ETA: 0s - loss: 0.8582 - acc: 0.586 - ETA: 0s - loss: 0.8437 - acc: 0.592 - ETA: 0s - loss: 0.8365 - acc: 0.603 - ETA: 0s - loss: 0.8302 - acc: 0.608 - ETA: 0s - loss: 0.8102 - acc: 0.631 - ETA: 0s - loss: 0.7906 - acc: 0.648 - ETA: 0s - loss: 0.7707 - acc: 0.657 - ETA: 0s - loss: 0.7579 - acc: 0.669 - ETA: 0s - loss: 0.7500 - acc: 0.673 - ETA: 0s - loss: 0.7464 - acc: 0.675 - ETA: 0s - loss: 0.7334 - acc: 0.685 - ETA: 0s - loss: 0.7153 - acc: 0.696 - ETA: 0s - loss: 0.7098 - acc: 0.702 - 1s 431us/step - loss: 0.7008 - acc: 0.7065 - val_loss: 2.6566 - val_acc: 0.0327\n",
      "Epoch 4/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 0.4961 - acc: 0.882 - ETA: 1s - loss: 0.4430 - acc: 0.898 - ETA: 1s - loss: 0.4303 - acc: 0.898 - ETA: 1s - loss: 0.4285 - acc: 0.889 - ETA: 1s - loss: 0.4335 - acc: 0.886 - ETA: 0s - loss: 0.4357 - acc: 0.884 - ETA: 0s - loss: 0.4298 - acc: 0.889 - ETA: 0s - loss: 0.4276 - acc: 0.887 - ETA: 0s - loss: 0.4267 - acc: 0.884 - ETA: 0s - loss: 0.4283 - acc: 0.886 - ETA: 0s - loss: 0.4246 - acc: 0.887 - ETA: 0s - loss: 0.4138 - acc: 0.889 - ETA: 0s - loss: 0.4069 - acc: 0.893 - ETA: 0s - loss: 0.4051 - acc: 0.893 - ETA: 0s - loss: 0.4033 - acc: 0.890 - ETA: 0s - loss: 0.3932 - acc: 0.893 - ETA: 0s - loss: 0.3841 - acc: 0.895 - ETA: 0s - loss: 0.3820 - acc: 0.894 - 1s 433us/step - loss: 0.3815 - acc: 0.8938 - val_loss: 3.5815 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 0.1877 - acc: 0.960 - ETA: 1s - loss: 0.2504 - acc: 0.918 - ETA: 1s - loss: 0.2586 - acc: 0.918 - ETA: 1s - loss: 0.2467 - acc: 0.921 - ETA: 0s - loss: 0.2498 - acc: 0.918 - ETA: 0s - loss: 0.2454 - acc: 0.919 - ETA: 0s - loss: 0.2451 - acc: 0.918 - ETA: 0s - loss: 0.2362 - acc: 0.919 - ETA: 0s - loss: 0.2347 - acc: 0.918 - ETA: 0s - loss: 0.2275 - acc: 0.921 - ETA: 0s - loss: 0.2279 - acc: 0.921 - ETA: 0s - loss: 0.2260 - acc: 0.921 - ETA: 0s - loss: 0.2237 - acc: 0.921 - ETA: 0s - loss: 0.2226 - acc: 0.922 - ETA: 0s - loss: 0.2209 - acc: 0.923 - ETA: 0s - loss: 0.2162 - acc: 0.925 - ETA: 0s - loss: 0.2146 - acc: 0.925 - ETA: 0s - loss: 0.2135 - acc: 0.926 - 1s 430us/step - loss: 0.2125 - acc: 0.9265 - val_loss: 3.8923 - val_acc: 0.0023\n",
      "Epoch 6/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 0.1440 - acc: 0.968 - ETA: 1s - loss: 0.1759 - acc: 0.933 - ETA: 1s - loss: 0.1690 - acc: 0.932 - ETA: 1s - loss: 0.1527 - acc: 0.946 - ETA: 1s - loss: 0.1629 - acc: 0.940 - ETA: 0s - loss: 0.1565 - acc: 0.940 - ETA: 0s - loss: 0.1522 - acc: 0.941 - ETA: 0s - loss: 0.1529 - acc: 0.942 - ETA: 0s - loss: 0.1510 - acc: 0.941 - ETA: 0s - loss: 0.1455 - acc: 0.944 - ETA: 0s - loss: 0.1461 - acc: 0.943 - ETA: 0s - loss: 0.1445 - acc: 0.944 - ETA: 0s - loss: 0.1454 - acc: 0.944 - ETA: 0s - loss: 0.1442 - acc: 0.945 - ETA: 0s - loss: 0.1404 - acc: 0.947 - ETA: 0s - loss: 0.1367 - acc: 0.947 - ETA: 0s - loss: 0.1397 - acc: 0.945 - ETA: 0s - loss: 0.1395 - acc: 0.946 - ETA: 0s - loss: 0.1335 - acc: 0.948 - 1s 435us/step - loss: 0.1332 - acc: 0.9492 - val_loss: 4.9263 - val_acc: 0.0152\n",
      "Epoch 7/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 0.0781 - acc: 0.968 - ETA: 1s - loss: 0.0779 - acc: 0.968 - ETA: 1s - loss: 0.0907 - acc: 0.962 - ETA: 1s - loss: 0.0942 - acc: 0.962 - ETA: 1s - loss: 0.0908 - acc: 0.966 - ETA: 0s - loss: 0.0977 - acc: 0.964 - ETA: 0s - loss: 0.0949 - acc: 0.966 - ETA: 0s - loss: 0.0922 - acc: 0.968 - ETA: 0s - loss: 0.0897 - acc: 0.967 - ETA: 0s - loss: 0.0901 - acc: 0.965 - ETA: 0s - loss: 0.0862 - acc: 0.967 - ETA: 0s - loss: 0.0884 - acc: 0.967 - ETA: 0s - loss: 0.0865 - acc: 0.968 - ETA: 0s - loss: 0.0859 - acc: 0.969 - ETA: 0s - loss: 0.0843 - acc: 0.970 - ETA: 0s - loss: 0.0829 - acc: 0.971 - ETA: 0s - loss: 0.0806 - acc: 0.972 - ETA: 0s - loss: 0.0808 - acc: 0.971 - ETA: 0s - loss: 0.0843 - acc: 0.970 - ETA: 0s - loss: 0.0836 - acc: 0.971 - ETA: 0s - loss: 0.0835 - acc: 0.971 - 2s 444us/step - loss: 0.0873 - acc: 0.9708 - val_loss: 4.6920 - val_acc: 0.1097\n",
      "Epoch 8/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 0.0690 - acc: 0.976 - ETA: 1s - loss: 0.0667 - acc: 0.979 - ETA: 1s - loss: 0.0700 - acc: 0.974 - ETA: 0s - loss: 0.0698 - acc: 0.979 - ETA: 0s - loss: 0.0698 - acc: 0.982 - ETA: 0s - loss: 0.0748 - acc: 0.979 - ETA: 0s - loss: 0.0689 - acc: 0.983 - ETA: 0s - loss: 0.0671 - acc: 0.983 - ETA: 0s - loss: 0.0660 - acc: 0.983 - ETA: 0s - loss: 0.0635 - acc: 0.984 - ETA: 0s - loss: 0.0642 - acc: 0.983 - ETA: 0s - loss: 0.0642 - acc: 0.982 - ETA: 0s - loss: 0.0645 - acc: 0.982 - ETA: 0s - loss: 0.0622 - acc: 0.983 - ETA: 0s - loss: 0.0599 - acc: 0.983 - ETA: 0s - loss: 0.0594 - acc: 0.983 - ETA: 0s - loss: 0.0587 - acc: 0.983 - 1s 429us/step - loss: 0.0590 - acc: 0.9834 - val_loss: 6.1625 - val_acc: 0.1692\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3428/3428 [==============================] - ETA: 1s - loss: 0.0252 - acc: 1.000 - ETA: 1s - loss: 0.0420 - acc: 0.992 - ETA: 1s - loss: 0.0443 - acc: 0.992 - ETA: 0s - loss: 0.0442 - acc: 0.993 - ETA: 0s - loss: 0.0420 - acc: 0.994 - ETA: 0s - loss: 0.0436 - acc: 0.992 - ETA: 0s - loss: 0.0425 - acc: 0.992 - ETA: 0s - loss: 0.0414 - acc: 0.992 - ETA: 0s - loss: 0.0405 - acc: 0.992 - ETA: 0s - loss: 0.0408 - acc: 0.992 - ETA: 0s - loss: 0.0413 - acc: 0.992 - ETA: 0s - loss: 0.0409 - acc: 0.992 - ETA: 0s - loss: 0.0388 - acc: 0.992 - ETA: 0s - loss: 0.0379 - acc: 0.992 - ETA: 0s - loss: 0.0375 - acc: 0.993 - ETA: 0s - loss: 0.0376 - acc: 0.992 - ETA: 0s - loss: 0.0367 - acc: 0.992 - 1s 429us/step - loss: 0.0373 - acc: 0.9921 - val_loss: 6.6754 - val_acc: 0.2007\n",
      "Epoch 10/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 0.0190 - acc: 1.000 - ETA: 1s - loss: 0.0188 - acc: 0.996 - ETA: 1s - loss: 0.0186 - acc: 0.997 - ETA: 1s - loss: 0.0190 - acc: 0.998 - ETA: 1s - loss: 0.0209 - acc: 0.996 - ETA: 0s - loss: 0.0192 - acc: 0.996 - ETA: 0s - loss: 0.0197 - acc: 0.996 - ETA: 0s - loss: 0.0186 - acc: 0.996 - ETA: 0s - loss: 0.0185 - acc: 0.996 - ETA: 0s - loss: 0.0190 - acc: 0.996 - ETA: 0s - loss: 0.0180 - acc: 0.996 - ETA: 0s - loss: 0.0200 - acc: 0.995 - ETA: 0s - loss: 0.0199 - acc: 0.996 - ETA: 0s - loss: 0.0201 - acc: 0.996 - ETA: 0s - loss: 0.0209 - acc: 0.995 - ETA: 0s - loss: 0.0203 - acc: 0.995 - ETA: 0s - loss: 0.0201 - acc: 0.995 - ETA: 0s - loss: 0.0197 - acc: 0.995 - ETA: 0s - loss: 0.0191 - acc: 0.995 - 2s 441us/step - loss: 0.0205 - acc: 0.9947 - val_loss: 7.1519 - val_acc: 0.1855\n",
      "Epoch 11/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 0.0068 - acc: 1.000 - ETA: 1s - loss: 0.0221 - acc: 0.997 - ETA: 1s - loss: 0.0268 - acc: 0.994 - ETA: 1s - loss: 0.0218 - acc: 0.996 - ETA: 1s - loss: 0.0215 - acc: 0.996 - ETA: 0s - loss: 0.0423 - acc: 0.988 - ETA: 0s - loss: 0.0407 - acc: 0.988 - ETA: 0s - loss: 0.0414 - acc: 0.988 - ETA: 0s - loss: 0.0501 - acc: 0.986 - ETA: 0s - loss: 0.0502 - acc: 0.987 - ETA: 0s - loss: 0.0490 - acc: 0.987 - ETA: 0s - loss: 0.0479 - acc: 0.987 - ETA: 0s - loss: 0.0469 - acc: 0.988 - ETA: 0s - loss: 0.0465 - acc: 0.988 - ETA: 0s - loss: 0.0459 - acc: 0.988 - ETA: 0s - loss: 0.0463 - acc: 0.988 - ETA: 0s - loss: 0.0451 - acc: 0.988 - ETA: 0s - loss: 0.0450 - acc: 0.988 - ETA: 0s - loss: 0.0446 - acc: 0.988 - 2s 442us/step - loss: 0.0440 - acc: 0.9886 - val_loss: 5.5655 - val_acc: 0.2229\n",
      "Epoch 12/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 0.0350 - acc: 0.976 - ETA: 1s - loss: 0.0414 - acc: 0.980 - ETA: 1s - loss: 0.0312 - acc: 0.987 - ETA: 1s - loss: 0.0262 - acc: 0.990 - ETA: 1s - loss: 0.0240 - acc: 0.992 - ETA: 1s - loss: 0.0225 - acc: 0.992 - ETA: 1s - loss: 0.0237 - acc: 0.992 - ETA: 0s - loss: 0.0217 - acc: 0.993 - ETA: 0s - loss: 0.0240 - acc: 0.992 - ETA: 0s - loss: 0.0228 - acc: 0.992 - ETA: 0s - loss: 0.0231 - acc: 0.992 - ETA: 0s - loss: 0.0278 - acc: 0.992 - ETA: 0s - loss: 0.0279 - acc: 0.992 - ETA: 0s - loss: 0.0279 - acc: 0.992 - ETA: 0s - loss: 0.0265 - acc: 0.993 - ETA: 0s - loss: 0.0258 - acc: 0.993 - ETA: 0s - loss: 0.0250 - acc: 0.993 - ETA: 0s - loss: 0.0245 - acc: 0.994 - ETA: 0s - loss: 0.0243 - acc: 0.994 - ETA: 0s - loss: 0.0235 - acc: 0.994 - ETA: 0s - loss: 0.0234 - acc: 0.994 - ETA: 0s - loss: 0.0234 - acc: 0.994 - ETA: 0s - loss: 0.0240 - acc: 0.994 - 2s 461us/step - loss: 0.0237 - acc: 0.9942 - val_loss: 5.7903 - val_acc: 0.2135\n",
      "Epoch 13/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 0.0043 - acc: 1.000 - ETA: 1s - loss: 0.0211 - acc: 0.996 - ETA: 1s - loss: 0.0129 - acc: 0.998 - ETA: 1s - loss: 0.0116 - acc: 0.998 - ETA: 1s - loss: 0.0110 - acc: 0.998 - ETA: 1s - loss: 0.0104 - acc: 0.998 - ETA: 0s - loss: 0.0101 - acc: 0.999 - ETA: 0s - loss: 0.0121 - acc: 0.998 - ETA: 0s - loss: 0.0118 - acc: 0.998 - ETA: 0s - loss: 0.0124 - acc: 0.998 - ETA: 0s - loss: 0.0124 - acc: 0.998 - ETA: 0s - loss: 0.0129 - acc: 0.997 - ETA: 0s - loss: 0.0135 - acc: 0.997 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0131 - acc: 0.997 - ETA: 0s - loss: 0.0126 - acc: 0.997 - ETA: 0s - loss: 0.0124 - acc: 0.997 - ETA: 0s - loss: 0.0121 - acc: 0.997 - ETA: 0s - loss: 0.0117 - acc: 0.997 - 2s 458us/step - loss: 0.0111 - acc: 0.9977 - val_loss: 6.2638 - val_acc: 0.1925\n",
      "Epoch 14/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 0.0012 - acc: 1.000 - ETA: 1s - loss: 0.0043 - acc: 1.000 - ETA: 1s - loss: 0.0038 - acc: 1.000 - ETA: 1s - loss: 0.0034 - acc: 1.000 - ETA: 1s - loss: 0.0115 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.998 - ETA: 0s - loss: 0.0088 - acc: 0.998 - ETA: 0s - loss: 0.0083 - acc: 0.998 - ETA: 0s - loss: 0.0126 - acc: 0.997 - ETA: 0s - loss: 0.0111 - acc: 0.997 - ETA: 0s - loss: 0.0105 - acc: 0.997 - ETA: 0s - loss: 0.0113 - acc: 0.997 - ETA: 0s - loss: 0.0132 - acc: 0.997 - ETA: 0s - loss: 0.0127 - acc: 0.997 - ETA: 0s - loss: 0.0123 - acc: 0.997 - ETA: 0s - loss: 0.0126 - acc: 0.996 - ETA: 0s - loss: 0.0122 - acc: 0.997 - ETA: 0s - loss: 0.0117 - acc: 0.997 - ETA: 0s - loss: 0.0119 - acc: 0.997 - ETA: 0s - loss: 0.0117 - acc: 0.997 - ETA: 0s - loss: 0.0131 - acc: 0.996 - ETA: 0s - loss: 0.0130 - acc: 0.997 - 2s 443us/step - loss: 0.0128 - acc: 0.9971 - val_loss: 5.4827 - val_acc: 0.2485\n",
      "Epoch 15/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 0.0164 - acc: 0.992 - ETA: 1s - loss: 0.0098 - acc: 0.994 - ETA: 1s - loss: 0.0081 - acc: 0.996 - ETA: 1s - loss: 0.0095 - acc: 0.995 - ETA: 1s - loss: 0.0088 - acc: 0.996 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0068 - acc: 0.997 - ETA: 0s - loss: 0.0070 - acc: 0.997 - ETA: 0s - loss: 0.0075 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0068 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.997 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0075 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0070 - acc: 0.997 - 2s 447us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 5.4200 - val_acc: 0.2345\n",
      "Epoch 16/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 0.0012 - acc: 1.000 - ETA: 1s - loss: 0.0027 - acc: 1.000 - ETA: 1s - loss: 0.0041 - acc: 1.000 - ETA: 1s - loss: 0.0037 - acc: 1.000 - ETA: 1s - loss: 0.0034 - acc: 1.000 - ETA: 0s - loss: 0.0034 - acc: 1.000 - ETA: 0s - loss: 0.0031 - acc: 1.000 - ETA: 0s - loss: 0.0031 - acc: 1.000 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.999 - ETA: 0s - loss: 0.0048 - acc: 0.999 - ETA: 0s - loss: 0.0046 - acc: 0.999 - ETA: 0s - loss: 0.0046 - acc: 0.999 - ETA: 0s - loss: 0.0045 - acc: 0.999 - ETA: 0s - loss: 0.0044 - acc: 0.999 - ETA: 0s - loss: 0.0051 - acc: 0.999 - ETA: 0s - loss: 0.0049 - acc: 0.999 - 1s 434us/step - loss: 0.0047 - acc: 0.9991 - val_loss: 6.0720 - val_acc: 0.2602\n",
      "Epoch 17/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 0.0018 - acc: 1.000 - ETA: 1s - loss: 0.0014 - acc: 1.000 - ETA: 1s - loss: 0.0015 - acc: 1.000 - ETA: 1s - loss: 0.0014 - acc: 1.000 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0037 - acc: 0.999 - ETA: 0s - loss: 0.0035 - acc: 0.999 - ETA: 0s - loss: 0.0040 - acc: 0.998 - ETA: 0s - loss: 0.0039 - acc: 0.998 - ETA: 0s - loss: 0.0037 - acc: 0.998 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0029 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0029 - acc: 0.999 - ETA: 0s - loss: 0.0029 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - 1s 435us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 6.7391 - val_acc: 0.2240\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3428/3428 [==============================] - ETA: 1s - loss: 0.0068 - acc: 1.000 - ETA: 1s - loss: 0.0027 - acc: 1.000 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0035 - acc: 0.998 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - ETA: 0s - loss: 0.0025 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0019 - acc: 0.999 - ETA: 0s - loss: 0.0018 - acc: 0.999 - ETA: 0s - loss: 0.0018 - acc: 0.999 - ETA: 0s - loss: 0.0017 - acc: 0.999 - ETA: 0s - loss: 0.0017 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0019 - acc: 0.999 - 1s 436us/step - loss: 0.0019 - acc: 0.9994 - val_loss: 6.9979 - val_acc: 0.2415\n",
      "Epoch 19/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 4.2212e-04 - acc: 1.000 - ETA: 1s - loss: 0.0031 - acc: 1.0000    - ETA: 1s - loss: 0.0025 - acc: 1.000 - ETA: 1s - loss: 0.0021 - acc: 1.000 - ETA: 0s - loss: 0.0017 - acc: 1.000 - ETA: 0s - loss: 0.0014 - acc: 1.000 - ETA: 0s - loss: 0.0013 - acc: 1.000 - ETA: 0s - loss: 0.0013 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0011 - acc: 1.000 - ETA: 0s - loss: 0.0011 - acc: 1.000 - ETA: 0s - loss: 0.0010 - acc: 1.000 - ETA: 0s - loss: 9.9819e-04 - acc: 1.000 - ETA: 0s - loss: 0.0014 - acc: 0.9997    - ETA: 0s - loss: 0.0014 - acc: 0.999 - 1s 414us/step - loss: 0.0013 - acc: 0.9997 - val_loss: 7.2314 - val_acc: 0.2404\n",
      "Epoch 20/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 4.4980e-04 - acc: 1.000 - ETA: 1s - loss: 6.0184e-04 - acc: 1.000 - ETA: 1s - loss: 0.0030 - acc: 0.9980    - ETA: 1s - loss: 0.0024 - acc: 0.998 - ETA: 0s - loss: 0.0019 - acc: 0.998 - ETA: 0s - loss: 0.0018 - acc: 0.999 - ETA: 0s - loss: 0.0017 - acc: 0.999 - ETA: 0s - loss: 0.0015 - acc: 0.999 - ETA: 0s - loss: 0.0014 - acc: 0.999 - ETA: 0s - loss: 0.0013 - acc: 0.999 - ETA: 0s - loss: 0.0012 - acc: 0.999 - ETA: 0s - loss: 0.0011 - acc: 0.999 - ETA: 0s - loss: 0.0010 - acc: 0.999 - ETA: 0s - loss: 0.0010 - acc: 0.999 - ETA: 0s - loss: 9.9791e-04 - acc: 0.999 - ETA: 0s - loss: 9.6602e-04 - acc: 0.999 - ETA: 0s - loss: 9.4061e-04 - acc: 0.999 - ETA: 0s - loss: 0.0011 - acc: 0.9997    - ETA: 0s - loss: 0.0011 - acc: 0.999 - 1s 433us/step - loss: 0.0010 - acc: 0.9997 - val_loss: 7.4556 - val_acc: 0.2334\n",
      "Epoch 21/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 4.0526e-04 - acc: 1.000 - ETA: 1s - loss: 3.4931e-04 - acc: 1.000 - ETA: 1s - loss: 4.9329e-04 - acc: 1.000 - ETA: 1s - loss: 5.0985e-04 - acc: 1.000 - ETA: 0s - loss: 5.1287e-04 - acc: 1.000 - ETA: 0s - loss: 6.8051e-04 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.0000    - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0011 - acc: 1.000 - ETA: 0s - loss: 9.5999e-04 - acc: 1.000 - ETA: 0s - loss: 9.2311e-04 - acc: 1.000 - ETA: 0s - loss: 8.5775e-04 - acc: 1.000 - ETA: 0s - loss: 8.1139e-04 - acc: 1.000 - ETA: 0s - loss: 7.6927e-04 - acc: 1.000 - ETA: 0s - loss: 7.6831e-04 - acc: 1.000 - ETA: 0s - loss: 7.4060e-04 - acc: 1.000 - 1s 432us/step - loss: 7.3057e-04 - acc: 1.0000 - val_loss: 7.7557 - val_acc: 0.2322\n",
      "Epoch 22/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 3.2056e-04 - acc: 1.000 - ETA: 1s - loss: 3.7459e-04 - acc: 1.000 - ETA: 1s - loss: 3.7880e-04 - acc: 1.000 - ETA: 1s - loss: 3.6708e-04 - acc: 1.000 - ETA: 1s - loss: 4.4953e-04 - acc: 1.000 - ETA: 0s - loss: 4.6946e-04 - acc: 1.000 - ETA: 0s - loss: 4.4850e-04 - acc: 1.000 - ETA: 0s - loss: 4.3396e-04 - acc: 1.000 - ETA: 0s - loss: 4.2480e-04 - acc: 1.000 - ETA: 0s - loss: 4.5024e-04 - acc: 1.000 - ETA: 0s - loss: 4.3800e-04 - acc: 1.000 - ETA: 0s - loss: 4.2004e-04 - acc: 1.000 - ETA: 0s - loss: 4.0819e-04 - acc: 1.000 - ETA: 0s - loss: 3.8506e-04 - acc: 1.000 - ETA: 0s - loss: 4.5073e-04 - acc: 1.000 - ETA: 0s - loss: 4.5308e-04 - acc: 1.000 - ETA: 0s - loss: 4.4232e-04 - acc: 1.000 - ETA: 0s - loss: 4.3165e-04 - acc: 1.000 - ETA: 0s - loss: 4.3241e-04 - acc: 1.000 - 2s 439us/step - loss: 4.3011e-04 - acc: 1.0000 - val_loss: 7.7333 - val_acc: 0.2462\n",
      "Epoch 23/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 4.3723e-04 - acc: 1.000 - ETA: 1s - loss: 4.4575e-04 - acc: 1.000 - ETA: 1s - loss: 3.7023e-04 - acc: 1.000 - ETA: 1s - loss: 3.9303e-04 - acc: 1.000 - ETA: 1s - loss: 4.5774e-04 - acc: 1.000 - ETA: 0s - loss: 4.2387e-04 - acc: 1.000 - ETA: 0s - loss: 3.9762e-04 - acc: 1.000 - ETA: 0s - loss: 3.8351e-04 - acc: 1.000 - ETA: 0s - loss: 3.7524e-04 - acc: 1.000 - ETA: 0s - loss: 3.6400e-04 - acc: 1.000 - ETA: 0s - loss: 3.7297e-04 - acc: 1.000 - ETA: 0s - loss: 3.6317e-04 - acc: 1.000 - ETA: 0s - loss: 3.5745e-04 - acc: 1.000 - ETA: 0s - loss: 3.4981e-04 - acc: 1.000 - ETA: 0s - loss: 3.4324e-04 - acc: 1.000 - ETA: 0s - loss: 3.3592e-04 - acc: 1.000 - ETA: 0s - loss: 3.3033e-04 - acc: 1.000 - ETA: 0s - loss: 3.2776e-04 - acc: 1.000 - ETA: 0s - loss: 3.2577e-04 - acc: 1.000 - ETA: 0s - loss: 3.3388e-04 - acc: 1.000 - 1s 436us/step - loss: 3.3475e-04 - acc: 1.0000 - val_loss: 7.8166 - val_acc: 0.2474\n",
      "Epoch 24/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 2.4575e-04 - acc: 1.000 - ETA: 1s - loss: 2.1250e-04 - acc: 1.000 - ETA: 1s - loss: 2.2487e-04 - acc: 1.000 - ETA: 1s - loss: 2.5452e-04 - acc: 1.000 - ETA: 1s - loss: 2.8793e-04 - acc: 1.000 - ETA: 1s - loss: 2.7373e-04 - acc: 1.000 - ETA: 0s - loss: 2.6319e-04 - acc: 1.000 - ETA: 0s - loss: 2.6401e-04 - acc: 1.000 - ETA: 0s - loss: 2.8013e-04 - acc: 1.000 - ETA: 0s - loss: 2.6869e-04 - acc: 1.000 - ETA: 0s - loss: 2.7077e-04 - acc: 1.000 - ETA: 0s - loss: 2.6993e-04 - acc: 1.000 - ETA: 0s - loss: 2.7048e-04 - acc: 1.000 - ETA: 0s - loss: 2.8272e-04 - acc: 1.000 - ETA: 0s - loss: 2.8426e-04 - acc: 1.000 - ETA: 0s - loss: 2.7623e-04 - acc: 1.000 - ETA: 0s - loss: 2.7639e-04 - acc: 1.000 - ETA: 0s - loss: 2.8397e-04 - acc: 1.000 - ETA: 0s - loss: 2.8836e-04 - acc: 1.000 - ETA: 0s - loss: 2.8811e-04 - acc: 1.000 - ETA: 0s - loss: 2.8755e-04 - acc: 1.000 - 2s 444us/step - loss: 2.8465e-04 - acc: 1.0000 - val_loss: 7.8781 - val_acc: 0.2462\n",
      "Epoch 25/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 2.2343e-04 - acc: 1.000 - ETA: 1s - loss: 2.0521e-04 - acc: 1.000 - ETA: 1s - loss: 2.2307e-04 - acc: 1.000 - ETA: 1s - loss: 2.3794e-04 - acc: 1.000 - ETA: 0s - loss: 2.4870e-04 - acc: 1.000 - ETA: 0s - loss: 2.5185e-04 - acc: 1.000 - ETA: 0s - loss: 2.5894e-04 - acc: 1.000 - ETA: 0s - loss: 2.5939e-04 - acc: 1.000 - ETA: 0s - loss: 2.6598e-04 - acc: 1.000 - ETA: 0s - loss: 2.7181e-04 - acc: 1.000 - ETA: 0s - loss: 2.6600e-04 - acc: 1.000 - ETA: 0s - loss: 2.6953e-04 - acc: 1.000 - ETA: 0s - loss: 2.6205e-04 - acc: 1.000 - ETA: 0s - loss: 2.6238e-04 - acc: 1.000 - ETA: 0s - loss: 2.5559e-04 - acc: 1.000 - ETA: 0s - loss: 2.5241e-04 - acc: 1.000 - ETA: 0s - loss: 2.4674e-04 - acc: 1.000 - ETA: 0s - loss: 2.4566e-04 - acc: 1.000 - ETA: 0s - loss: 2.4934e-04 - acc: 1.000 - 1s 436us/step - loss: 2.5449e-04 - acc: 1.0000 - val_loss: 7.9319 - val_acc: 0.2474\n",
      "Epoch 26/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 1.4520e-04 - acc: 1.000 - ETA: 1s - loss: 2.0919e-04 - acc: 1.000 - ETA: 1s - loss: 2.1571e-04 - acc: 1.000 - ETA: 1s - loss: 2.3652e-04 - acc: 1.000 - ETA: 1s - loss: 2.3525e-04 - acc: 1.000 - ETA: 0s - loss: 2.2554e-04 - acc: 1.000 - ETA: 0s - loss: 2.2334e-04 - acc: 1.000 - ETA: 0s - loss: 2.1777e-04 - acc: 1.000 - ETA: 0s - loss: 2.1995e-04 - acc: 1.000 - ETA: 0s - loss: 2.2944e-04 - acc: 1.000 - ETA: 0s - loss: 2.2500e-04 - acc: 1.000 - ETA: 0s - loss: 2.2333e-04 - acc: 1.000 - ETA: 0s - loss: 2.2209e-04 - acc: 1.000 - ETA: 0s - loss: 2.2627e-04 - acc: 1.000 - ETA: 0s - loss: 2.2607e-04 - acc: 1.000 - ETA: 0s - loss: 2.3143e-04 - acc: 1.000 - ETA: 0s - loss: 2.3039e-04 - acc: 1.000 - ETA: 0s - loss: 2.3201e-04 - acc: 1.000 - 1s 429us/step - loss: 2.3031e-04 - acc: 1.0000 - val_loss: 7.9740 - val_acc: 0.2485\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3428/3428 [==============================] - ETA: 1s - loss: 2.2300e-04 - acc: 1.000 - ETA: 1s - loss: 2.1098e-04 - acc: 1.000 - ETA: 1s - loss: 2.0385e-04 - acc: 1.000 - ETA: 1s - loss: 2.0011e-04 - acc: 1.000 - ETA: 0s - loss: 1.9732e-04 - acc: 1.000 - ETA: 0s - loss: 2.0370e-04 - acc: 1.000 - ETA: 0s - loss: 2.0617e-04 - acc: 1.000 - ETA: 0s - loss: 2.0620e-04 - acc: 1.000 - ETA: 0s - loss: 2.0833e-04 - acc: 1.000 - ETA: 0s - loss: 2.1279e-04 - acc: 1.000 - ETA: 0s - loss: 2.1116e-04 - acc: 1.000 - ETA: 0s - loss: 2.1343e-04 - acc: 1.000 - ETA: 0s - loss: 2.0846e-04 - acc: 1.000 - ETA: 0s - loss: 2.0903e-04 - acc: 1.000 - ETA: 0s - loss: 2.1010e-04 - acc: 1.000 - ETA: 0s - loss: 2.1146e-04 - acc: 1.000 - ETA: 0s - loss: 2.0968e-04 - acc: 1.000 - 2s 438us/step - loss: 2.0918e-04 - acc: 1.0000 - val_loss: 8.0438 - val_acc: 0.2474\n",
      "Epoch 28/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 1.6515e-04 - acc: 1.000 - ETA: 1s - loss: 1.7087e-04 - acc: 1.000 - ETA: 1s - loss: 1.9659e-04 - acc: 1.000 - ETA: 1s - loss: 1.8732e-04 - acc: 1.000 - ETA: 0s - loss: 1.8006e-04 - acc: 1.000 - ETA: 0s - loss: 1.7527e-04 - acc: 1.000 - ETA: 0s - loss: 1.8868e-04 - acc: 1.000 - ETA: 0s - loss: 1.8799e-04 - acc: 1.000 - ETA: 0s - loss: 1.8799e-04 - acc: 1.000 - ETA: 0s - loss: 1.9770e-04 - acc: 1.000 - ETA: 0s - loss: 1.9830e-04 - acc: 1.000 - ETA: 0s - loss: 1.9694e-04 - acc: 1.000 - ETA: 0s - loss: 1.9671e-04 - acc: 1.000 - ETA: 0s - loss: 1.9356e-04 - acc: 1.000 - ETA: 0s - loss: 1.9123e-04 - acc: 1.000 - ETA: 0s - loss: 1.9299e-04 - acc: 1.000 - ETA: 0s - loss: 1.9224e-04 - acc: 1.000 - ETA: 0s - loss: 1.9065e-04 - acc: 1.000 - 1s 427us/step - loss: 1.9215e-04 - acc: 1.0000 - val_loss: 8.0948 - val_acc: 0.2474\n",
      "Epoch 29/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 2.1138e-04 - acc: 1.000 - ETA: 1s - loss: 1.6534e-04 - acc: 1.000 - ETA: 0s - loss: 1.5955e-04 - acc: 1.000 - ETA: 0s - loss: 1.7092e-04 - acc: 1.000 - ETA: 0s - loss: 1.7779e-04 - acc: 1.000 - ETA: 0s - loss: 1.7272e-04 - acc: 1.000 - ETA: 0s - loss: 1.7455e-04 - acc: 1.000 - ETA: 0s - loss: 1.7710e-04 - acc: 1.000 - ETA: 0s - loss: 1.7484e-04 - acc: 1.000 - ETA: 0s - loss: 1.7141e-04 - acc: 1.000 - ETA: 0s - loss: 1.7166e-04 - acc: 1.000 - ETA: 0s - loss: 1.7140e-04 - acc: 1.000 - ETA: 0s - loss: 1.7381e-04 - acc: 1.000 - ETA: 0s - loss: 1.7519e-04 - acc: 1.000 - ETA: 0s - loss: 1.7801e-04 - acc: 1.000 - ETA: 0s - loss: 1.7570e-04 - acc: 1.000 - ETA: 0s - loss: 1.7387e-04 - acc: 1.000 - ETA: 0s - loss: 1.7838e-04 - acc: 1.000 - 1s 432us/step - loss: 1.7761e-04 - acc: 1.0000 - val_loss: 8.1386 - val_acc: 0.2462\n",
      "Epoch 30/30\n",
      "3428/3428 [==============================] - ETA: 1s - loss: 1.8874e-04 - acc: 1.000 - ETA: 1s - loss: 1.6574e-04 - acc: 1.000 - ETA: 1s - loss: 1.5538e-04 - acc: 1.000 - ETA: 1s - loss: 1.5132e-04 - acc: 1.000 - ETA: 0s - loss: 1.6110e-04 - acc: 1.000 - ETA: 0s - loss: 1.6221e-04 - acc: 1.000 - ETA: 0s - loss: 1.6626e-04 - acc: 1.000 - ETA: 0s - loss: 1.6070e-04 - acc: 1.000 - ETA: 0s - loss: 1.5744e-04 - acc: 1.000 - ETA: 0s - loss: 1.5522e-04 - acc: 1.000 - ETA: 0s - loss: 1.6344e-04 - acc: 1.000 - ETA: 0s - loss: 1.6093e-04 - acc: 1.000 - ETA: 0s - loss: 1.6373e-04 - acc: 1.000 - ETA: 0s - loss: 1.6242e-04 - acc: 1.000 - ETA: 0s - loss: 1.6519e-04 - acc: 1.000 - ETA: 0s - loss: 1.6507e-04 - acc: 1.000 - ETA: 0s - loss: 1.6434e-04 - acc: 1.000 - ETA: 0s - loss: 1.6503e-04 - acc: 1.000 - 1s 433us/step - loss: 1.6510e-04 - acc: 1.0000 - val_loss: 8.1728 - val_acc: 0.2485\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 32\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "\n",
    "#model.add(Flatten())\n",
    "model.add(LSTM(100))\n",
    "#model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X, y,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "name": "loss",
         "type": "scatter",
         "uid": "55e93180-9181-4acf-a524-ecff92e0a838",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29
         ],
         "xaxis": "x",
         "y": [
          1.2798709374186317,
          1.1218266699945634,
          0.7007672433396899,
          0.381516207509447,
          0.2124793764242909,
          0.13317124041334194,
          0.08731978375338618,
          0.05903010115531668,
          0.03725613508473478,
          0.020514916367785376,
          0.04396892449837026,
          0.02366296476994227,
          0.01110539970052284,
          0.012778639217975573,
          0.007131417620983605,
          0.004702836717397426,
          0.0026599643130508516,
          0.0018516615830669386,
          0.0013418719838096248,
          0.0010404402730810427,
          0.0007305703621168589,
          0.0004301136642799885,
          0.0003347520929010848,
          0.00028465430632539535,
          0.00025449020822984554,
          0.00023031137329977236,
          0.00020917729609290002,
          0.00019215065657183998,
          0.00017760950617794853,
          0.00016509854122550774
         ],
         "yaxis": "y"
        },
        {
         "name": "val_loss",
         "type": "scatter",
         "uid": "890b7d4a-7c0e-4838-a54b-d59769dd39dc",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29
         ],
         "xaxis": "x",
         "y": [
          2.913434514921493,
          2.5857021346809983,
          2.656611831074139,
          3.5814945491915404,
          3.892329919852899,
          4.9262689659448045,
          4.692043806875044,
          6.162496273826412,
          6.675414772545582,
          7.151923537949837,
          5.565549623173343,
          5.790332772489746,
          6.263761591048891,
          5.482671852011664,
          5.420034752982639,
          6.071954194139016,
          6.739114612236602,
          6.997911252385677,
          7.23139302065659,
          7.455571763852096,
          7.7556729010689915,
          7.733301271774666,
          7.816628903463535,
          7.878128199760944,
          7.931936976790011,
          7.974049040706461,
          8.043820465181366,
          8.094772870788217,
          8.138561632140792,
          8.172778067816932
         ],
         "yaxis": "y"
        },
        {
         "name": "acc",
         "type": "scatter",
         "uid": "8ee2e11c-0ae5-42bb-a357-21e7f3771a3e",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29
         ],
         "xaxis": "x2",
         "y": [
          0.4049008170114515,
          0.41277712976501313,
          0.7065344230992374,
          0.893815636078424,
          0.9264877478884427,
          0.9492415408131122,
          0.9708284713423516,
          0.9833722295393883,
          0.9921236875594149,
          0.9947491251323437,
          0.9886231041288431,
          0.9941656942823804,
          0.9976662777129521,
          0.9970828471411902,
          0.9979579929988331,
          0.9991248541423571,
          0.9994165694282381,
          0.9994165694282381,
          0.999708284714119,
          0.999708284714119,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "yaxis": "y2"
        },
        {
         "name": "val_acc",
         "type": "scatter",
         "uid": "43e36715-e4df-47d6-bdca-5c429c4c763b",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29
         ],
         "xaxis": "x2",
         "y": [
          0,
          0.017502917164812774,
          0.03267211217515805,
          0,
          0.002333722287047841,
          0.015169194804954418,
          0.10968494712610924,
          0.1691948673063009,
          0.20070011651223849,
          0.18553092220283027,
          0.2228704779609916,
          0.21353558992560576,
          0.19253208934217517,
          0.2485414228403165,
          0.23453908967443218,
          0.2602100365359419,
          0.2240373396609183,
          0.24154025625737435,
          0.24037339511385042,
          0.23337222853090828,
          0.23220536683098159,
          0.24620770027506728,
          0.24737456141859118,
          0.24620770027506728,
          0.24737456141859118,
          0.24854142256211512,
          0.24737456141859118,
          0.24737456141859118,
          0.24620770027506728,
          0.24854142256211512
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "title": {
         "text": "Loss | Accuracy"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ]
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"68a6a2da-9748-4e94-86a6-ba53f17b205f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"68a6a2da-9748-4e94-86a6-ba53f17b205f\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '68a6a2da-9748-4e94-86a6-ba53f17b205f',\n",
       "                        [{\"name\": \"loss\", \"type\": \"scatter\", \"uid\": \"55e93180-9181-4acf-a524-ecff92e0a838\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], \"xaxis\": \"x\", \"y\": [1.2798709374186317, 1.1218266699945634, 0.7007672433396899, 0.381516207509447, 0.2124793764242909, 0.13317124041334194, 0.08731978375338618, 0.05903010115531668, 0.03725613508473478, 0.020514916367785376, 0.04396892449837026, 0.02366296476994227, 0.01110539970052284, 0.012778639217975573, 0.007131417620983605, 0.004702836717397426, 0.0026599643130508516, 0.0018516615830669386, 0.0013418719838096248, 0.0010404402730810427, 0.0007305703621168589, 0.0004301136642799885, 0.0003347520929010848, 0.00028465430632539535, 0.00025449020822984554, 0.00023031137329977236, 0.00020917729609290002, 0.00019215065657183998, 0.00017760950617794853, 0.00016509854122550774], \"yaxis\": \"y\"}, {\"name\": \"val_loss\", \"type\": \"scatter\", \"uid\": \"890b7d4a-7c0e-4838-a54b-d59769dd39dc\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], \"xaxis\": \"x\", \"y\": [2.913434514921493, 2.5857021346809983, 2.656611831074139, 3.5814945491915404, 3.892329919852899, 4.9262689659448045, 4.692043806875044, 6.162496273826412, 6.675414772545582, 7.151923537949837, 5.565549623173343, 5.790332772489746, 6.263761591048891, 5.482671852011664, 5.420034752982639, 6.071954194139016, 6.739114612236602, 6.997911252385677, 7.23139302065659, 7.455571763852096, 7.7556729010689915, 7.733301271774666, 7.816628903463535, 7.878128199760944, 7.931936976790011, 7.974049040706461, 8.043820465181366, 8.094772870788217, 8.138561632140792, 8.172778067816932], \"yaxis\": \"y\"}, {\"name\": \"acc\", \"type\": \"scatter\", \"uid\": \"8ee2e11c-0ae5-42bb-a357-21e7f3771a3e\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], \"xaxis\": \"x2\", \"y\": [0.4049008170114515, 0.41277712976501313, 0.7065344230992374, 0.893815636078424, 0.9264877478884427, 0.9492415408131122, 0.9708284713423516, 0.9833722295393883, 0.9921236875594149, 0.9947491251323437, 0.9886231041288431, 0.9941656942823804, 0.9976662777129521, 0.9970828471411902, 0.9979579929988331, 0.9991248541423571, 0.9994165694282381, 0.9994165694282381, 0.999708284714119, 0.999708284714119, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], \"yaxis\": \"y2\"}, {\"name\": \"val_acc\", \"type\": \"scatter\", \"uid\": \"43e36715-e4df-47d6-bdca-5c429c4c763b\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], \"xaxis\": \"x2\", \"y\": [0.0, 0.017502917164812774, 0.03267211217515805, 0.0, 0.002333722287047841, 0.015169194804954418, 0.10968494712610924, 0.1691948673063009, 0.20070011651223849, 0.18553092220283027, 0.2228704779609916, 0.21353558992560576, 0.19253208934217517, 0.2485414228403165, 0.23453908967443218, 0.2602100365359419, 0.2240373396609183, 0.24154025625737435, 0.24037339511385042, 0.23337222853090828, 0.23220536683098159, 0.24620770027506728, 0.24737456141859118, 0.24620770027506728, 0.24737456141859118, 0.24854142256211512, 0.24737456141859118, 0.24737456141859118, 0.24620770027506728, 0.24854142256211512], \"yaxis\": \"y2\"}],\n",
       "                        {\"title\": {\"text\": \"Loss | Accuracy\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 0.45]}, \"xaxis2\": {\"anchor\": \"y2\", \"domain\": [0.55, 1.0]}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0]}, \"yaxis2\": {\"anchor\": \"x2\", \"domain\": [0.0, 1.0]}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('68a6a2da-9748-4e94-86a6-ba53f17b205f');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = pd.DataFrame(history.history)\n",
    "\n",
    "loss = go.Scatter(\n",
    "    x = history.index,\n",
    "    y = history.loss,\n",
    "    name = 'loss'\n",
    ")\n",
    "\n",
    "val_loss = go.Scatter(\n",
    "    x = history.index,\n",
    "    y = history.val_loss,\n",
    "    name = 'val_loss'\n",
    ")\n",
    "\n",
    "acc = go.Scatter(\n",
    "    x = history.index,\n",
    "    y = history.acc,\n",
    "    name = 'acc'\n",
    ")\n",
    "\n",
    "val_acc = go.Scatter(\n",
    "    x = history.index,\n",
    "    y = history.val_acc,\n",
    "    name = 'val_acc'\n",
    ")\n",
    "\n",
    "fig = tools.make_subplots(rows=1, cols=2)\n",
    "\n",
    "fig.append_trace(loss, 1, 1)\n",
    "fig.append_trace(val_loss, 1, 1)\n",
    "\n",
    "fig.append_trace(acc, 1, 2)\n",
    "fig.append_trace(val_acc, 1, 2)\n",
    "\n",
    "fig['layout'].update(title='Loss | Accuracy')\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
